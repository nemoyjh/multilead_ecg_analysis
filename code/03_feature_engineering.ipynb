{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "regulated-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.signal as signal\n",
    "import peakutils\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "import ast\n",
    "import neurokit2 as nk\n",
    "\n",
    "#For signal manipulation and fast fourier transform\n",
    "import scipy.signal as signal\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-teaching",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering\n",
    "\n",
    "This notebook creates the features that are used as input for the Cascading Classifier. As described in the main README file, the following types of features are created:\n",
    "\n",
    "Statistical features: Kurtosis, used to detect signal quality, and Skew, used to detect symmetry around the R-peak.\n",
    "\n",
    "Power and Frequency features: Fast-fourier transformed data (frequency domain), for phase and power features, and Short-time fourier transformed data (time-frequency domain), for frequency and location features.\n",
    "\n",
    "Amplitude/Voltage features: Max, Min, and Avg peak heights, as the amplitude on the ECG paper represents the amount of voltage.\n",
    "\n",
    "This notebook is split into several parts.\n",
    "\n",
    "1. [Loading the data](#part-3-1-loading-the-data)\n",
    "2. [Creating features](#part-3-2-creating-features)\n",
    " 1. [Calculating skew and kurtosis](#part-3-2-1-calculating-skew-and-kurtosis)\n",
    " 2. [Calculating short-time fourier transform](#part-3-2-2-calculating-short-time-fourier-transform)\n",
    " 3. [Calculating fast fourier transform](#part-3-2-3-calculating-fast-fourier-transform)\n",
    " 4. [Calculating highest, lowest, and average amplitude of the heartbeats and taking their mean](#part-3-2-4-calculating-highest-lowest-and-average-amplitude-of-the-heartbeats-and-taking-their-mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-badge",
   "metadata": {},
   "source": [
    "<a id=part-3-1-loading-the-data></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-valve",
   "metadata": {},
   "source": [
    "### Part 3.1: Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-gallery",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "circular-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw_signals in array format\n",
    "raw_signals = np.load('../data/data_analysis_files/ptb_raw_signals.npz')\n",
    "raw_signals = raw_signals['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "higher-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.119, -0.055,  0.064, ..., -0.026, -0.039, -0.079],\n",
       "        [-0.116, -0.051,  0.065, ..., -0.031, -0.034, -0.074],\n",
       "        [-0.12 , -0.044,  0.076, ..., -0.028, -0.029, -0.069],\n",
       "        ...,\n",
       "        [ 0.069,  0.   , -0.069, ...,  0.024, -0.041, -0.058],\n",
       "        [ 0.086,  0.004, -0.081, ...,  0.242, -0.046, -0.098],\n",
       "        [ 0.022, -0.031, -0.054, ...,  0.143, -0.035, -0.12 ]]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_signals[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "united-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw_signals that were converted to dataframe\n",
    "signals_df = pd.read_csv('../data/data_analysis_files/ptb_signals_df.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "weekly-coach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  \\\n",
       "0 -0.119 -0.055  0.064  0.086 -0.091  0.004 -0.069 -0.031  0.000 -0.026   \n",
       "1 -0.116 -0.051  0.065  0.083 -0.090  0.006 -0.064 -0.036 -0.003 -0.031   \n",
       "2 -0.120 -0.044  0.076  0.082 -0.098  0.016 -0.058 -0.034 -0.010 -0.028   \n",
       "3 -0.117 -0.038  0.080  0.077 -0.098  0.021 -0.050 -0.030 -0.015 -0.023   \n",
       "4 -0.103 -0.031  0.072  0.066 -0.087  0.021 -0.045 -0.027 -0.020 -0.019   \n",
       "\n",
       "      10     11  \n",
       "0 -0.039 -0.079  \n",
       "1 -0.034 -0.074  \n",
       "2 -0.029 -0.069  \n",
       "3 -0.022 -0.064  \n",
       "4 -0.018 -0.058  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "historic-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_df = pd.read_csv('../data/data_analysis_files/meta_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "faced-beatles",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>nurse</th>\n",
       "      <th>site</th>\n",
       "      <th>device</th>\n",
       "      <th>recording_date</th>\n",
       "      <th>report</th>\n",
       "      <th>...</th>\n",
       "      <th>baseline_drift</th>\n",
       "      <th>static_noise</th>\n",
       "      <th>burst_noise</th>\n",
       "      <th>electrodes_problems</th>\n",
       "      <th>extra_beats</th>\n",
       "      <th>pacemaker</th>\n",
       "      <th>strat_fold</th>\n",
       "      <th>filename_lr</th>\n",
       "      <th>filename_hr</th>\n",
       "      <th>diagnostic_superclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15709.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1984-11-09 09:17:34</td>\n",
       "      <td>sinusrhythmus periphere niederspannung</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>, I-V1,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>records100/00000/00001_lr</td>\n",
       "      <td>records500/00000/00001_hr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13243.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1984-11-14 12:55:37</td>\n",
       "      <td>sinusbradykardie sonst normales ekg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>records100/00000/00002_lr</td>\n",
       "      <td>records500/00000/00002_hr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20372.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1984-11-15 12:49:10</td>\n",
       "      <td>sinusrhythmus normales ekg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>records100/00000/00003_lr</td>\n",
       "      <td>records500/00000/00003_hr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17014.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1984-11-15 13:44:57</td>\n",
       "      <td>sinusrhythmus normales ekg</td>\n",
       "      <td>...</td>\n",
       "      <td>, II,III,AVF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>records100/00000/00004_lr</td>\n",
       "      <td>records500/00000/00004_hr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17448.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CS-12   E</td>\n",
       "      <td>1984-11-17 10:43:15</td>\n",
       "      <td>sinusrhythmus normales ekg</td>\n",
       "      <td>...</td>\n",
       "      <td>, III,AVR,AVF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>records100/00000/00005_lr</td>\n",
       "      <td>records500/00000/00005_hr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id   age  sex  height  weight  nurse  site     device  \\\n",
       "0     15709.0  56.0    1     NaN    63.0    2.0   0.0  CS-12   E   \n",
       "1     13243.0  19.0    0     NaN    70.0    2.0   0.0  CS-12   E   \n",
       "2     20372.0  37.0    1     NaN    69.0    2.0   0.0  CS-12   E   \n",
       "3     17014.0  24.0    0     NaN    82.0    2.0   0.0  CS-12   E   \n",
       "4     17448.0  19.0    1     NaN    70.0    2.0   0.0  CS-12   E   \n",
       "\n",
       "        recording_date                                  report  ...  \\\n",
       "0  1984-11-09 09:17:34  sinusrhythmus periphere niederspannung  ...   \n",
       "1  1984-11-14 12:55:37     sinusbradykardie sonst normales ekg  ...   \n",
       "2  1984-11-15 12:49:10              sinusrhythmus normales ekg  ...   \n",
       "3  1984-11-15 13:44:57              sinusrhythmus normales ekg  ...   \n",
       "4  1984-11-17 10:43:15              sinusrhythmus normales ekg  ...   \n",
       "\n",
       "   baseline_drift static_noise burst_noise electrodes_problems  extra_beats  \\\n",
       "0             NaN    , I-V1,           NaN                 NaN          NaN   \n",
       "1             NaN          NaN         NaN                 NaN          NaN   \n",
       "2             NaN          NaN         NaN                 NaN          NaN   \n",
       "3    , II,III,AVF          NaN         NaN                 NaN          NaN   \n",
       "4   , III,AVR,AVF          NaN         NaN                 NaN          NaN   \n",
       "\n",
       "   pacemaker  strat_fold                filename_lr  \\\n",
       "0        NaN           3  records100/00000/00001_lr   \n",
       "1        NaN           2  records100/00000/00002_lr   \n",
       "2        NaN           5  records100/00000/00003_lr   \n",
       "3        NaN           3  records100/00000/00004_lr   \n",
       "4        NaN           4  records100/00000/00005_lr   \n",
       "\n",
       "                 filename_hr diagnostic_superclass  \n",
       "0  records500/00000/00001_hr                  NORM  \n",
       "1  records500/00000/00002_hr                  NORM  \n",
       "2  records500/00000/00003_hr                  NORM  \n",
       "3  records500/00000/00004_hr                  NORM  \n",
       "4  records500/00000/00005_hr                  NORM  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-camcorder",
   "metadata": {},
   "source": [
    "<a id=part-3-2-creating-features></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-maria",
   "metadata": {},
   "source": [
    "### Part 3.2: Creating features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-locator",
   "metadata": {},
   "source": [
    "<a id=part-3-2-1-calculating-skew-and-kurtosis></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-substance",
   "metadata": {},
   "source": [
    "#### Part 3.2.1: Calculating Skew and Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-yacht",
   "metadata": {},
   "source": [
    "Let's calculate the skewness of one of the channels of an ecg. Apply a scipy skew function to a rolling window of the whole 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "civilian-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the scipy function and window size to be used\n",
    "skew_func = sp.stats.skew\n",
    "window = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "proud-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2.350041\n",
      "1     1.826513\n",
      "2    -0.833393\n",
      "3    -2.266357\n",
      "4     2.084855\n",
      "5     0.757849\n",
      "6    -4.019669\n",
      "7    -3.866994\n",
      "8    -0.825758\n",
      "9     2.247386\n",
      "10    3.170848\n",
      "11    1.842737\n",
      "Name: 999, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Applying the skew function to ecg 0, whole 10 seconds, first lead, which is raw_signals[0][0:999][0]\n",
    "#or signals_df[0:1000]\n",
    "skew_first_ecg = pd.DataFrame.rolling(signals_df[0:1000],window).skew\n",
    "print(skew_first_ecg().iloc[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-dallas",
   "metadata": {},
   "source": [
    "Define a new function to apply functions like mean, skew, and other function names and return the results in a dataframe for that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "accurate-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_on_ecg(dataframe, function_name='skew'):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with 21837000 rows and 12 columns, with each row being a time step and each column being a lead\n",
    "    of the ecg reading\n",
    "    Output: A dataframe with 21837 rows and 12 columns, with each row being one ecg and each column being a lead\n",
    "    of the ecg reading. Each value will be the applied function's values, e.g. 'mean' will retrieve the mean\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(data=None)\n",
    "    \n",
    "    # For each of the 12-leads, do...\n",
    "    for i in range(12):\n",
    "    \n",
    "        # Convert the dataframe into a (1000,21837) dataframe, with each column being 1 ecg for the 1000 time steps\n",
    "        changed_df = pd.DataFrame(signals_df.iloc[:,i].values.reshape(21837,1000)).T\n",
    "        \n",
    "        #Apply the function per variable\n",
    "        ecg_values = getattr(changed_df, function_name)()\n",
    "        \n",
    "        #Turn the above output_array into a dataframe for concatenation\n",
    "        df_for_appending = pd.DataFrame(ecg_values)\n",
    "        \n",
    "        #Concatenate df_for_appending to the output_df\n",
    "        output_df = pd.concat([output_df, df_for_appending], axis=1)\n",
    "    \n",
    "    #Rename columns with lead number\n",
    "    output_df.columns = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    \n",
    "    return output_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "arbitrary-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the kurtosis function\n",
    "kurtosis_df = func_on_ecg(signals_df, 'kurtosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "tamil-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save kurtosis_df as a csv\n",
    "kurtosis_df.to_csv('../data/features_df_folder/kurtosis_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "atmospheric-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the skew function\n",
    "skew_df = func_on_ecg(signals_df, 'skew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "addressed-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save skew_df as a csv\n",
    "skew_df.to_csv('../data/features_df_folder/skew_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-segment",
   "metadata": {},
   "source": [
    "<a id=part-3-2-2-calculating-short-time-fourier-transform></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-birmingham",
   "metadata": {},
   "source": [
    "#### Part 3.2.2: Calculating Short-Time Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-blood",
   "metadata": {},
   "source": [
    "Let's define a function to get the short-time fourier transform for each patient in the dataframe. Sum up these values so that it works as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "rental-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_on_df(dataframe):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with 21837000 rows and 12 columns, with each row being a time step and each column being a lead\n",
    "    of the ecg reading\n",
    "    Output: A dataframe with 21837 rows and 12 columns, with each row being one ecg and each column being a lead\n",
    "    of the ecg reading. Values in this dataframe will be the sum of the short-time fourier transform, which can be\n",
    "    used as a feature\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(data=None)\n",
    "    \n",
    "    # For each of the 12-leads, do...\n",
    "    for i in range(12):\n",
    "    \n",
    "        # Convert the dataframe into a (1000,21837) dataframe, with each column being 1 ecg for the 1000 time steps\n",
    "        changed_df = pd.DataFrame(signals_df.iloc[:,i].values.reshape(21837,1000)).T\n",
    "        \n",
    "        #Apply the signal_timefrequency function from neurokit2 for each column. Access the 2nd value, which\n",
    "        #is an array of the short-time fourier transform values\n",
    "        functioned_df = changed_df.apply(lambda x : nk.signal_timefrequency(x, show = False), axis=0).iloc[2]\n",
    "        \n",
    "        #Initiate empty array to store the sum for the next step\n",
    "        output_array = []\n",
    "        \n",
    "        #Sum up the short-time fourier transform values\n",
    "        [output_array.append(np.sum(functioned_df[j])) for j in range(21837)]\n",
    "        \n",
    "        #Turn the above output_array into a dataframe for concatenation\n",
    "        df_for_appending = pd.DataFrame(output_array)\n",
    "        \n",
    "        #Concatenate df_for_appending to the output_df\n",
    "        output_df = pd.concat([output_df, df_for_appending], axis=1)\n",
    "    \n",
    "    #Rename columns with lead number\n",
    "    output_df.columns = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    \n",
    "    return output_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "portuguese-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junho\\anaconda3\\lib\\site-packages\\scipy\\signal\\spectral.py:1961: UserWarning: nperseg = 50000 is greater than input length  = 1000, using nperseg = 1000\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "#Applying the stft_on_df function and saving the output to stft_df\n",
    "stft_df = stft_on_df(signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "accomplished-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving stft_df to csv\n",
    "stft_df.to_csv('../data/features_df_folder/stft_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-still",
   "metadata": {},
   "source": [
    "<a id=part-3-2-3-calculating-fast-fourier-transform></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-perception",
   "metadata": {},
   "source": [
    "#### Part 3.2.3: Calculating Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "disturbed-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_on_df(dataframe):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with 21837000 rows and 12 columns, with each row being a time step and each column being a lead\n",
    "    of the ecg reading\n",
    "    Output: A dataframe with 21837 rows and 12 columns, with each row being one ecg and each column being a lead\n",
    "    of the ecg reading. Values in this dataframe will be the sum of the fast fourier transform, which can be\n",
    "    used as a feature\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(data=None)\n",
    "    \n",
    "    # For each of the 12-leads, do...\n",
    "    for i in range(12):\n",
    "    \n",
    "        # Convert the dataframe into a (1000,21837) dataframe, with each column being 1 ecg for the 1000 time steps\n",
    "        changed_df = pd.DataFrame(signals_df.iloc[:,i].values.reshape(21837,1000)).T\n",
    "        \n",
    "        #Apply the signal_resample function from neurokit2 for each column. Access the 2nd value, which\n",
    "        #is an array of the fast fourier transform values\n",
    "        functioned_df = changed_df.apply(lambda x : nk.signal_resample(signal=x, \n",
    "                                                                       sampling_rate=100, \n",
    "                                                                       desired_sampling_rate=100,\n",
    "                                                                       method='fft'), axis=0).iloc[2]\n",
    "        \n",
    "        #Initiate empty array to store the sum for the next step\n",
    "        output_array = []\n",
    "        \n",
    "        #Sum up the fast fourier transform values\n",
    "        [output_array.append(np.sum(functioned_df[j])) for j in range(21837)]\n",
    "        \n",
    "        #Turn the above output_array into a dataframe for concatenation\n",
    "        df_for_appending = pd.DataFrame(output_array)\n",
    "        \n",
    "        #Concatenate df_for_appending to the output_df\n",
    "        output_df = pd.concat([output_df, df_for_appending], axis=1)\n",
    "    \n",
    "    #Rename columns with lead number\n",
    "    output_df.columns = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "subject-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the fft_on_df function\n",
    "fft_df = fft_on_df(signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "friendly-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving fft_df to csv\n",
    "fft_df.to_csv('../data/features_df_folder/fft_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-pressing",
   "metadata": {},
   "source": [
    "<a id=part-3-2-4-calculating-highest-lowest-and-average-amplitude-of-the-heartbeats-and-taking-their-mean></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-jurisdiction",
   "metadata": {},
   "source": [
    "#### Part 3.2.4: Calculating highest, lowest, and average amplitude of the heartbeats and taking their mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-special",
   "metadata": {},
   "source": [
    "The amplitudes of each heartbeat represents the voltage going through the heart. Calculating the highest amplitude (normally the same as the R-peak, though not always; see channel 7 ('V2')) and lowest amplitude in a heartbeat will give a representation of the strength of the voltage. The mean, highest, and lowest amplitudes will be taken to give an overall value for the ecg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-december",
   "metadata": {},
   "source": [
    "Let's calculate the maximum amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "residential-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_peak_values(dataframe):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with 21837000 rows and 12 columns, with each row being a time step and each column being a lead\n",
    "    of the ecg reading. \n",
    "    Output: A dataframe with 21837 rows and 12 columns, with each row being one ecg and each column being a lead\n",
    "    of the ecg reading. Values in this dataframe will be the sum of the maximum peak values.\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(data=None)\n",
    "\n",
    "    for i in range(12):\n",
    "        # Convert the dataframe into a (1000,21837) dataframe, with each column being 1 ecg for the 1000 time steps\n",
    "        changed_df = pd.DataFrame(dataframe.iloc[:,i].values.reshape(21837,1000)).T\n",
    "\n",
    "        #Get the max values. Use index 0 to get the list.\n",
    "        sample_max_index = changed_df.apply(lambda x: signal.find_peaks(x, distance= 90)[0])\n",
    "\n",
    "        #For a column (a particular ecg), get the values of the sample_max_index values for that column.\n",
    "        #Then, sum up the values together.\n",
    "        sample_max_values = [sum(changed_df.iloc[sample_max_index[j],column_index]) for column_index, j\\\n",
    "                             in enumerate(range(len(sample_max_index)))]\n",
    "\n",
    "        #Convert the array to a dataframe\n",
    "        df_for_appending = pd.DataFrame(sample_max_values)\n",
    "\n",
    "        #Concatenate df_for_appending to the output_df\n",
    "        output_df = pd.concat([output_df, df_for_appending], axis=1)\n",
    "    \n",
    "    #Rename columns with lead number\n",
    "    output_df.columns = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    \n",
    "    return (output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "diverse-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply max_peak_values function\n",
    "max_peak_df = max_peak_values(signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fuzzy-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save max_peak_df to csv\n",
    "max_peak_df.to_csv('../data/features_df_folder/max_peak_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-belgium",
   "metadata": {},
   "source": [
    "Now let's invert signals_df and get the minima values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "selective-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiply signals_df by -1 to invert the signals\n",
    "inverted_signals_df = signals_df * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "favorite-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_peak_values(dataframe, inverted_dataframe):\n",
    "    \"\"\"\n",
    "    Input: A dataframe with 21837000 rows and 12 columns, with each row being a time step and each column being a lead\n",
    "    of the ecg reading. An inverted dataframe with the same shape as the first dataframe and is simply the inverse of\n",
    "    the original dataframe.\n",
    "    Output: A dataframe with 21837 rows and 12 columns, with each row being one ecg and each column being a lead\n",
    "    of the ecg reading. Values in this dataframe will be the sum of the minimum peak values.\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(data=None)\n",
    "\n",
    "    for i in range(12):\n",
    "        # Convert the inverted dataframe into a (1000,21837) dataframe, \n",
    "        # with each column being 1 ecg for the 1000 time steps\n",
    "        changed_inverted_df = pd.DataFrame(inverted_dataframe.iloc[:,i].values.reshape(21837,1000)).T\n",
    "\n",
    "        # Convert the dataframe into a (1000,21837) dataframe, with each column being 1 ecg for the 1000 time steps\n",
    "        changed_df = pd.DataFrame(inverted_dataframe.iloc[:,i].values.reshape(21837,1000)).T\n",
    "        \n",
    "        #Get the min values from inverted_dataframe. Use index 0 to get the list.\n",
    "        sample_min_index = changed_inverted_df.apply(lambda x: signal.find_peaks(x, distance= 90)[0])\n",
    "\n",
    "        #For a column (a particular ecg), get the values of the sample_min_index values for that column.\n",
    "        #Then, sum up the values together.\n",
    "        sample_min_values = [sum(changed_df.iloc[sample_min_index[j],column_index]) for column_index, j\\\n",
    "                             in enumerate(range(len(sample_min_index)))]\n",
    "\n",
    "        #Convert the array to a dataframe\n",
    "        df_for_appending = pd.DataFrame(sample_min_values)\n",
    "\n",
    "        #Concatenate df_for_appending to the output_df\n",
    "        output_df = pd.concat([output_df, df_for_appending], axis=1)\n",
    "    \n",
    "    #Rename columns with lead number\n",
    "    output_df.columns = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    \n",
    "    return (output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "forbidden-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply min_peak_values on inverted signals\n",
    "min_peak_df = min_peak_values(signals_df, inverted_signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "healthy-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert min_peak_df to csv\n",
    "min_peak_df.to_csv('../data/features_df_folder/min_peak_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "frozen-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average peaks\n",
    "avg_peak_df = (max_peak_df - min_peak_df) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "rational-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save avg_peak_df to csv\n",
    "avg_peak_df.to_csv('../data/features_df_folder/avg_peak_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
